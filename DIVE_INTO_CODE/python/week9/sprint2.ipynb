{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習スクラッチ 畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    def __init__(self, N_in=np.array([1,2,3,4]), padding=0, filter_size=np.array([3, 5, 7]), stride=1):\n",
    "        self.w = filter_size\n",
    "        self.b = np.array([1])\n",
    "        self.x = N_in\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.N_out = self.compute_out(N_in=self.x, padding=self.padding, filter_size=self.w, stride=self.stride)\n",
    "        \n",
    "    def compute_out(self, N_in, padding, filter_size, stride):\n",
    "        N_out_shape = (len(N_in) + 2 * padding - len(filter_size)) / stride + 1\n",
    "        return np.zeros(int(N_out_shape))\n",
    "        \n",
    "    def forward(self):\n",
    "        for i in range(len(self.N_out)):\n",
    "            self.N_out[i] = np.sum(self.x[0+i:len(self.w)+i] * self.w, axis=0) + 1\n",
    "            \n",
    "    def backward(self, loss=np.array([10, 20])):\n",
    "        self.delta_w = np.zeros(self.w.size)\n",
    "        self.delta_x = np.zeros(self.x.size)\n",
    "        self.delta_b = np.sum(loss)\n",
    "        for i in range(self.w.size):\n",
    "            self.delta_w[i] = np.sum(loss * self.x[0+i:len(loss)+i])\n",
    "            \n",
    "        for j in range(self.x.size):\n",
    "            for s in range(self.w.size):\n",
    "                loss_index = j - s\n",
    "                if loss_index < 0 or loss_index > self.N_out.size - 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.delta_x[j] += loss[loss_index] * self.w[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out(N_in, padding, filter_size, stride):\n",
    "    return (N_in + 2 * padding - filter_size) / stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "計算された出力サイズ：8.0\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "N_in = 10\n",
    "padding = 0\n",
    "filter_size = 3\n",
    "stride = 1\n",
    "print(f'計算された出力サイズ：{compute_out(N_in, padding, filter_size, stride)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35. 50.]\n",
      "[ 50.  80. 110.] 30 [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "simple_comv = SimpleConv1d()\n",
    "\n",
    "# 順伝搬確認\n",
    "simple_comv.forward()\n",
    "print(simple_comv.N_out)\n",
    "\n",
    "# 逆伝搬確認\n",
    "simple_comv.backward()\n",
    "print(simple_comv.delta_w, simple_comv.delta_b, simple_comv.delta_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiConv1d:\n",
    "    def __init__(self, N_in=None, filter_size=None, stride=1, padding=0):\n",
    "        self.w = filter_size\n",
    "        self.b = np.array([1, 2, 3]) # 題意に沿って設定\n",
    "        self.x = N_in\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.N_out = np.zeros((self.w.shape[0], self.w.shape[1]))\n",
    "        \n",
    "    def forward(self):\n",
    "        for i in range(self.N_out.shape[0]):\n",
    "            for j in range(self.N_out.shape[1]):\n",
    "                self.N_out[i, j] = np.sum(self.w[i].reshape(-1) * self.x[:, j:self.w[i].shape[1]+j].reshape(-1)) + self.b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n"
     ]
    }
   ],
   "source": [
    "multi = MultiConv1d(N_in=x, filter_size=w)\n",
    "\n",
    "# 順伝搬確認\n",
    "multi.forward()\n",
    "print(multi.N_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題5】（アドバンス課題）パディングの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題6】（アドバンス課題）ミニバッチへの対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題7】（アドバンス課題）任意のストライド数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes2)\n",
    "        \n",
    "    \n",
    "class SimpleConv1d:\n",
    "    # ここに入るn_nodes1:バッチサイズ×画素数（20 * 784）\n",
    "    def __init__(self, stride=1, padding=0):\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = 7681\n",
    "        self.W = np.random.rand(self.filter_size)\n",
    "        self.B = np.array([1])\n",
    "        \n",
    "    def compute_out(self, N_in, padding, filter_size, stride):\n",
    "        N_out_shape = (len(N_in) + 2 * padding - filter_size) / stride + 1\n",
    "        return np.zeros(int(N_out_shape))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.x = X\n",
    "        N_out = self.compute_out(X, self.padding, self.filter_size, self.stride)\n",
    "        for i in range(len(N_out)):\n",
    "            N_out[i] = np.sum(self.x[0+i:self.W.shape[0]+i] * self.W, axis=0) + 1\n",
    "        return N_out\n",
    "            \n",
    "    def backward(self, dA):\n",
    "        self.delta_w = np.zeros(self.W.size)\n",
    "        self.delta_x = np.zeros(self.x.size)\n",
    "        self.delta_b = np.sum(dA)\n",
    "        self.B = self.delta_b\n",
    "        for i in range(self.W.size):\n",
    "            self.delta_w[i] = np.sum(dA * self.x[0+i:len(dA)+i])\n",
    "            self.W[i] = self.delta_w[i]\n",
    "            \n",
    "#         for j in range(self.x.size):\n",
    "#             for s in range(self.W.size):\n",
    "#                 loss_index = j - s\n",
    "#                 if loss_index < 0 or loss_index > dA.size - 1:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     self.delta_x[j] += dA[loss_index] * self.W[s]\n",
    "                    \n",
    "        dZ = self.delta_x\n",
    "        return dZ\n",
    "    \n",
    "class Xavier:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), n_nodes2)\n",
    "    \n",
    "class He:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), n_nodes2)\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_z, layer_a : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "    \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H_i = 0\n",
    "        self.B_i = 0\n",
    "    def update(self, layer_z, layer_a, layer):\n",
    "        self.H_i += np.dot(layer_z, layer_a) ** 2\n",
    "        self.B_i += layer_a.sum(axis=0) ** 2\n",
    "        layer.W -= self.lr * np.sqrt(1 / self.H_i) * layer_z\n",
    "        layer.B -= self.lr * np.sqrt(1 / self.B_i) * layer_a.sum(axis=0)\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\" \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class actsoftmax:\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculates log(sum(exp(x))).\n",
    "        \"\"\"\n",
    "        xmax = x.max(axis=1, keepdims=True)\n",
    "        z = np.log(np.exp(x - xmax).sum(axis=1, keepdims=True)) + xmax\n",
    "        Z = x - z\n",
    "        self.Z = np.exp(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, Y, Z):\n",
    "        batch_size = Y.shape[0]\n",
    "        return (Z - Y) / batch_size\n",
    "\n",
    "    def loss_func(self, y, z):\n",
    "        if y.ndim == 1:\n",
    "            z = z.reshape(1, z.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        return -(y * z).mean(axis=0).sum()\n",
    "    \n",
    "class actsigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        bck_sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - bck_sig) * bck_sig\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "class acttanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "    \n",
    "class actrelu:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
    "        \n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden1_size=400, hidden2_size=200, output_size=10,\n",
    "                sigma=0.01, batch_size=20, lr=0.01, verbose=True, act=acttanh, opt=None):\n",
    "        self.verbose = verbose\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.act = act\n",
    "        self.opt = opt\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # イテレーション毎の順伝搬\n",
    "        # 入力1層目が1dの為、reshapeをかける\n",
    "        self.x = X.reshape(-1)\n",
    "        self.A1 = self.FC1.forward(self.x)\n",
    "        A1 = self.A1.reshape(self.batch_size, -1)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "    \n",
    "    def backward(self, y):\n",
    "        # イテレーション毎の逆伝搬\n",
    "        dA3 = self.activation3.backward(y, self.activation3.Z) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        # 逆伝搬最終層は1dの為、reshapeをかける\n",
    "        self.dA1 = dA1.reshape(-1)\n",
    "        dZ0 = self.FC1.backward(self.dA1) # dZ0は使用しない\n",
    "    \n",
    "    # 正解率を出力\n",
    "    def accuracy(self, y, z):\n",
    "        return (z.argmax(axis=1) == y).sum()\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=20):\n",
    "        \n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.hidden1_size : 1層目のノード数\n",
    "        # self.hidden2_size : 2層目のノード数\n",
    "        # self.output_size : 出力層のノード数\n",
    "        # 各層の設定\n",
    "        # optimizerの設定\n",
    "        if self.opt == None:\n",
    "            optimizer = SGD(self.lr)\n",
    "            \n",
    "        elif self.opt == 'Ada':\n",
    "            optimizer = AdaGrad(self.lr)\n",
    "            \n",
    "        # activateの設定\n",
    "        if self.act == acttanh:\n",
    "            initial = SimpleInitializer(self.sigma)\n",
    "            \n",
    "        elif self.act == actsigmoid:\n",
    "            initial = Xavier(self.sigma)\n",
    "            \n",
    "        elif self.act == actrelu:\n",
    "            initial = He(self.sigma)\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.FC1 = SimpleConv1d(stride=1, padding=0)\n",
    "        self.activation1 = self.act() # tanh\n",
    "        self.FC2 = FC(self.hidden1_size, self.hidden2_size, initial, optimizer)\n",
    "        self.activation2 = self.act() # tanh\n",
    "        self.FC3 = FC(self.hidden2_size, self.output_size, initial, optimizer)\n",
    "        self.activation3 = actsoftmax() # Softmax\n",
    "        div_iter = 600\n",
    "        plot_data = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            val_get_mini_batch = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "            for i in range(len(get_mini_batch)):\n",
    "                X_train, y_train = get_mini_batch[i]\n",
    "                X_val2, y_val2 = val_get_mini_batch[i]\n",
    "                batch_size = X_train.shape[0]\n",
    "                self.X = X_train.reshape(batch_size, 784)\n",
    "                self.y = (y_train.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "\n",
    "#                 print(self.activation3.Z)\n",
    "                Z3 = self.forward(self.X)\n",
    "                loss = self.activation3.loss_func(self.y, Z3)\n",
    "#                 print(f'{i+1} / {len(get_mini_batch)} | train loss:{loss}')\n",
    "                self.backward(self.y)\n",
    "                \n",
    "#                 self.y_val = (y_val2.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "#                 self.X_val = X_val2.reshape(batch_size, 784)\n",
    "#                 val_Z3 = self.forward(self.X_val)\n",
    "#                 val_loss = self.activation3.loss_func(self.y_val, val_Z3)\n",
    "#                 val_accuracy = self.accuracy(y_val2, self.activation3.Z)\n",
    "                    \n",
    "                print(f'epoch: {epoch + 1} / {epochs}, iteration: {i + 1} / {len(get_mini_batch)} | train loss : {loss:.3}')\n",
    "                    \n",
    "                iters_per_epoch = len(X) / self.batch_size\n",
    "                plot_data.append((epoch + (i + 1) / iters_per_epoch, loss))\n",
    "                    \n",
    "        if self.verbose:\n",
    "#             epochs, train_loss, val_loss = zip(*plot_data)\n",
    "            epochs, train_loss = zip(*plot_data)\n",
    "            plt.plot(epochs, train_loss, color='g', label='train loss')\n",
    "#             plt.plot(epochs, val_loss, color='b', label='val loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "                \n",
    "    def predict(self, X, y):\n",
    "        test_get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "        result_array = np.array([])\n",
    "        for i in range(len(test_get_mini_batch)):\n",
    "            X_test, _ = test_get_mini_batch[i]\n",
    "            self.X_test = X_test.reshape(self.batch_size, 784)\n",
    "            Z3 = self.forward(self.X)\n",
    "            if len(result_array) == 0:\n",
    "                result_array = self.activation3.Z\n",
    "            else:\n",
    "                result_array = np.append(result_array, self.activation3.Z)\n",
    "                \n",
    "        result_array = result_array.reshape(-1, 10)\n",
    "            \n",
    "        return np.argmax(result_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ処理のサンプルクラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "(48000, 784) (12000, 784)\n",
      "(48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "# MNISTダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 用意した画像データセットを(サンプル数, 一次元の画素数)型に変換\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# 画素値を正規化処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.min(), X_train.max())\n",
    "print(X_test.min(), X_test.max())\n",
    "\n",
    "# trainとvalデータに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 3 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 4 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 5 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 6 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 7 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 8 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 9 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 10 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 11 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 12 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 13 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 14 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 15 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 16 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 17 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 18 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 19 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 20 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 21 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 22 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 23 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 24 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 25 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 26 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 27 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 28 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 29 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 30 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 31 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 32 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 33 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 34 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 35 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 36 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 37 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 38 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 39 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 40 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 41 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 42 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 43 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 44 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 45 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 46 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 47 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 48 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 49 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 50 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 51 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 52 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 53 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 54 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 55 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 56 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 57 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 58 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 59 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 60 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 61 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 62 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 63 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 64 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 65 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 66 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 67 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 68 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 69 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 70 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 71 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 72 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 73 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 74 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 75 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 76 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 77 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 78 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 79 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 80 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 81 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 82 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 83 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 84 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 85 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 86 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 87 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 88 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 89 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 90 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 91 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 92 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 93 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 94 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 95 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 96 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 97 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 98 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 99 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 100 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 101 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 102 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 103 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 104 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 105 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 106 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 107 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 108 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 109 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 110 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 111 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 112 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 113 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 114 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 115 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 116 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 117 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 118 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 119 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 120 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 121 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 122 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 123 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 124 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 125 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 126 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 127 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 128 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 129 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 130 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 131 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 132 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 133 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 134 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 135 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 136 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 137 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 138 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 139 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 140 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 141 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 142 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 143 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 144 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 145 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 146 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 147 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 148 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 149 / 2400 | train loss : 2.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 150 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 151 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 152 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 153 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 154 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 155 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 156 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 157 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 158 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 159 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 160 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 161 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 162 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 163 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 164 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 165 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 166 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 167 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 168 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 169 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 170 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 171 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 172 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 173 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 174 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 175 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 176 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 177 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 178 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 179 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 180 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 181 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 182 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 183 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 184 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 185 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 186 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 187 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 188 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 189 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 190 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 191 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 192 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 193 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 194 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 195 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 196 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 197 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 198 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 199 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 200 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 201 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 202 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 203 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 204 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 205 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 206 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 207 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 208 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 209 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 210 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 211 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 212 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 213 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 214 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 215 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 216 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 217 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 218 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 219 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 220 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 221 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 222 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 223 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 224 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 225 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 226 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 227 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 228 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 229 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 230 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 231 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 232 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 233 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 234 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 235 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 236 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 237 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 238 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 239 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 240 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 241 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 242 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 243 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 244 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 245 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 246 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 247 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 248 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 249 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 250 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 251 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 252 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 253 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 254 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 255 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 256 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 257 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 258 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 259 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 260 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 261 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 262 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 263 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 264 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 265 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 266 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 267 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 268 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 269 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 270 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 271 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 272 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 273 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 274 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 275 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 276 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 277 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 278 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 279 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 280 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 281 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 282 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 283 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 284 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 285 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 286 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 287 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 288 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 289 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 290 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 291 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 292 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 293 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 294 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 295 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 296 / 2400 | train loss : 2.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 297 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 298 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 299 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 300 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 301 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 302 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 303 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 304 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 305 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 306 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 307 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 308 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 309 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 310 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 311 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 312 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 313 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 314 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 315 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 316 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 317 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 318 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 319 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 320 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 321 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 322 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 323 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 324 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 325 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 326 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 327 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 328 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 329 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 330 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 331 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 332 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 333 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 334 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 335 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 336 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 337 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 338 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 339 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 340 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 341 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 342 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 343 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 344 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 345 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 346 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 347 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 348 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 349 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 350 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 351 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 352 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 353 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 354 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 355 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 356 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 357 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 358 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 359 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 360 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 361 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 362 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 363 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 364 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 365 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 366 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 367 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 368 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 369 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 370 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 371 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 372 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 373 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 374 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 375 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 376 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 377 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 378 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 379 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 380 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 381 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 382 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 383 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 384 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 385 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 386 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 387 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 388 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 389 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 390 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 391 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 392 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 393 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 394 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 395 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 396 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 397 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 398 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 399 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 400 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 401 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 402 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 403 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 404 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 405 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 406 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 407 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 408 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 409 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 410 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 411 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 412 / 2400 | train loss : 2.2\n",
      "epoch: 1 / 1, iteration: 413 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 414 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 415 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 416 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 417 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 418 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 419 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 420 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 421 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 422 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 423 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 424 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 425 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 426 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 427 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 428 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 429 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 430 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 431 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 432 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 433 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 434 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 435 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 436 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 437 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 438 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 439 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 440 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 441 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 442 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 443 / 2400 | train loss : 2.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 444 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 445 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 446 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 447 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 448 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 449 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 450 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 451 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 452 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 453 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 454 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 455 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 456 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 457 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 458 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 459 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 460 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 461 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 462 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 463 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 464 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 465 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 466 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 467 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 468 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 469 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 470 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 471 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 472 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 473 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 474 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 475 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 476 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 477 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 478 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 479 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 480 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 481 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 482 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 483 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 484 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 485 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 486 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 487 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 488 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 489 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 490 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 491 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 492 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 493 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 494 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 495 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 496 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 497 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 498 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 499 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 500 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 501 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 502 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 503 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 504 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 505 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 506 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 507 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 508 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 509 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 510 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 511 / 2400 | train loss : 2.48\n",
      "epoch: 1 / 1, iteration: 512 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 513 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 514 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 515 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 516 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 517 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 518 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 519 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 520 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 521 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 522 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 523 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 524 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 525 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 526 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 527 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 528 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 529 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 530 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 531 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 532 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 533 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 534 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 535 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 536 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 537 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 538 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 539 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 540 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 541 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 542 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 543 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 544 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 545 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 546 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 547 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 548 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 549 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 550 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 551 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 552 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 553 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 554 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 555 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 556 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 557 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 558 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 559 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 560 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 561 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 562 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 563 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 564 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 565 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 566 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 567 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 568 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 569 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 570 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 571 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 572 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 573 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 574 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 575 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 576 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 577 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 578 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 579 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 580 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 581 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 582 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 583 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 584 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 585 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 586 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 587 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 588 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 589 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 590 / 2400 | train loss : 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 591 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 592 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 593 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 594 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 595 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 596 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 597 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 598 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 599 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 600 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 601 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 602 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 603 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 604 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 605 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 606 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 607 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 608 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 609 / 2400 | train loss : 2.2\n",
      "epoch: 1 / 1, iteration: 610 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 611 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 612 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 613 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 614 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 615 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 616 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 617 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 618 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 619 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 620 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 621 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 622 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 623 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 624 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 625 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 626 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 627 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 628 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 629 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 630 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 631 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 632 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 633 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 634 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 635 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 636 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 637 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 638 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 639 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 640 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 641 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 642 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 643 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 644 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 645 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 646 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 647 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 648 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 649 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 650 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 651 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 652 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 653 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 654 / 2400 | train loss : 2.49\n",
      "epoch: 1 / 1, iteration: 655 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 656 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 657 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 658 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 659 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 660 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 661 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 662 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 663 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 664 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 665 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 666 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 667 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 668 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 669 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 670 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 671 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 672 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 673 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 674 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 675 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 676 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 677 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 678 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 679 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 680 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 681 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 682 / 2400 | train loss : 2.19\n",
      "epoch: 1 / 1, iteration: 683 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 684 / 2400 | train loss : 2.49\n",
      "epoch: 1 / 1, iteration: 685 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 686 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 687 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 688 / 2400 | train loss : 2.16\n",
      "epoch: 1 / 1, iteration: 689 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 690 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 691 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 692 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 693 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 694 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 695 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 696 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 697 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 698 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 699 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 700 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 701 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 702 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 703 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 704 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 705 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 706 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 707 / 2400 | train loss : 2.2\n",
      "epoch: 1 / 1, iteration: 708 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 709 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 710 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 711 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 712 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 713 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 714 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 715 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 716 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 717 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 718 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 719 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 720 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 721 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 722 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 723 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 724 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 725 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 726 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 727 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 728 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 729 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 730 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 731 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 732 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 733 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 734 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 735 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 736 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 737 / 2400 | train loss : 2.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 738 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 739 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 740 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 741 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 742 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 743 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 744 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 745 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 746 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 747 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 748 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 749 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 750 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 751 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 752 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 753 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 754 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 755 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 756 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 757 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 758 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 759 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 760 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 761 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 762 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 763 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 764 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 765 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 766 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 767 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 768 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 769 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 770 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 771 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 772 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 773 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 774 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 775 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 776 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 777 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 778 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 779 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 780 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 781 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 782 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 783 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 784 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 785 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 786 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 787 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 788 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 789 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 790 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 791 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 792 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 793 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 794 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 795 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 796 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 797 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 798 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 799 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 800 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 801 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 802 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 803 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 804 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 805 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 806 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 807 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 808 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 809 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 810 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 811 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 812 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 813 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 814 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 815 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 816 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 817 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 818 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 819 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 820 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 821 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 822 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 823 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 824 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 825 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 826 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 827 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 828 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 829 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 830 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 831 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 832 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 833 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 834 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 835 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 836 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 837 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 838 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 839 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 840 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 841 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 842 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 843 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 844 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 845 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 846 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 847 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 848 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 849 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 850 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 851 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 852 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 853 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 854 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 855 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 856 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 857 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 858 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 859 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 860 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 861 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 862 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 863 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 864 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 865 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 866 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 867 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 868 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 869 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 870 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 871 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 872 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 873 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 874 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 875 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 876 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 877 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 878 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 879 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 880 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 881 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 882 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 883 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 884 / 2400 | train loss : 2.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 885 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 886 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 887 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 888 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 889 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 890 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 891 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 892 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 893 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 894 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 895 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 896 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 897 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 898 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 899 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 900 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 901 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 902 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 903 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 904 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 905 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 906 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 907 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 908 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 909 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 910 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 911 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 912 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 913 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 914 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 915 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 916 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 917 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 918 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 919 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 920 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 921 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 922 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 923 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 924 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 925 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 926 / 2400 | train loss : 2.47\n",
      "epoch: 1 / 1, iteration: 927 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 928 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 929 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 930 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 931 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 932 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 933 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 934 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 935 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 936 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 937 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 938 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 939 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 940 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 941 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 942 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 943 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 944 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 945 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 946 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 947 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 948 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 949 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 950 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 951 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 952 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 953 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 954 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 955 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 956 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 957 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 958 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 959 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 960 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 961 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 962 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 963 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 964 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 965 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 966 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 967 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 968 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 969 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 970 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 971 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 972 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 973 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 974 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 975 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 976 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 977 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 978 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 979 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 980 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 981 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 982 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 983 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 984 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 985 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 986 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 987 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 988 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 989 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 990 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 991 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 992 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 993 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 994 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 995 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 996 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 997 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 998 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 999 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1000 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1001 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1002 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1003 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1004 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1005 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1006 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1007 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1008 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1009 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1010 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1011 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1012 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1013 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1014 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1015 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1016 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 1017 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 1018 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1019 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1020 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1021 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1022 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1023 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1024 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1025 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1026 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1027 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1028 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1029 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1030 / 2400 | train loss : 2.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1031 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1032 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1033 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1034 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1035 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1036 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1037 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1038 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1039 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1040 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1041 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1042 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1043 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1044 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1045 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1046 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1047 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1048 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1049 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1050 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1051 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1052 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1053 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1054 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1055 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1056 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1057 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1058 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1059 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1060 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1061 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1062 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1063 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1064 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1065 / 2400 | train loss : 2.16\n",
      "epoch: 1 / 1, iteration: 1066 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1067 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1068 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1069 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1070 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1071 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 1072 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1073 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1074 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1075 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1076 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1077 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1078 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1079 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1080 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1081 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1082 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1083 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1084 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1085 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1086 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1087 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1088 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1089 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1090 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1091 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1092 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1093 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1094 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1095 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1096 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1097 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1098 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1099 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1100 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1101 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1102 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1103 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1104 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1105 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1106 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1107 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1108 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1109 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1110 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1111 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1112 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1113 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1114 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1115 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1116 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1117 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1118 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1119 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1120 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1121 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1122 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1123 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1124 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1125 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1126 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1127 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1128 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1129 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1130 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1131 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1132 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1133 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1134 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1135 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1136 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1137 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1138 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1139 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1140 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1141 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1142 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1143 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1144 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1145 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1146 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1147 / 2400 | train loss : 2.2\n",
      "epoch: 1 / 1, iteration: 1148 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1149 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1150 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1151 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1152 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1153 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1154 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1155 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1156 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1157 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1158 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1159 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1160 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1161 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1162 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1163 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1164 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1165 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1166 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1167 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1168 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 1169 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1170 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1171 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1172 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1173 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1174 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1175 / 2400 | train loss : 2.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1176 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1177 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1178 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1179 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1180 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1181 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1182 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1183 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1184 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1185 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1186 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1187 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1188 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1189 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1190 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1191 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1192 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1193 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1194 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1195 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1196 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1197 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1198 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1199 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1200 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 1201 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1202 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1203 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1204 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1205 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1206 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1207 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1208 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1209 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1210 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1211 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1212 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1213 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1214 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1215 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1216 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1217 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1218 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1219 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1220 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1221 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1222 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1223 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1224 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1225 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1226 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1227 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1228 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1229 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1230 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1231 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1232 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1233 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1234 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1235 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1236 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1237 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1238 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1239 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1240 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1241 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1242 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1243 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1244 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1245 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 1246 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1247 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1248 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1249 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1250 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1251 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1252 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1253 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1254 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1255 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1256 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1257 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1258 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1259 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1260 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1261 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1262 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1263 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1264 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1265 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1266 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1267 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1268 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1269 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1270 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1271 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1272 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1273 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1274 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1275 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1276 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1277 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1278 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1279 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1280 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1281 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1282 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1283 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1284 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1285 / 2400 | train loss : 2.44\n",
      "epoch: 1 / 1, iteration: 1286 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1287 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1288 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1289 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1290 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1291 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1292 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1293 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 1294 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1295 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1296 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1297 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1298 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1299 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1300 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1301 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1302 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1303 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1304 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1305 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1306 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1307 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1308 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1309 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1310 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1311 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1312 / 2400 | train loss : 2.17\n",
      "epoch: 1 / 1, iteration: 1313 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1314 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1315 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1316 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 1317 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1318 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1319 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1320 / 2400 | train loss : 2.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1321 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1322 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1323 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1324 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1325 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1326 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1327 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1328 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1329 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1330 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1331 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1332 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1333 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1334 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1335 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1336 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1337 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1338 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1339 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1340 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1341 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1342 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1343 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1344 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1345 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1346 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1347 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1348 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1349 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1350 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1351 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1352 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1353 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1354 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1355 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1356 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1357 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 1358 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1359 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1360 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1361 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1362 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1363 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1364 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1365 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1366 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1367 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1368 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1369 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1370 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1371 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1372 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1373 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1374 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1375 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1376 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1377 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1378 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1379 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1380 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1381 / 2400 | train loss : 2.2\n",
      "epoch: 1 / 1, iteration: 1382 / 2400 | train loss : 2.2\n",
      "epoch: 1 / 1, iteration: 1383 / 2400 | train loss : 2.52\n",
      "epoch: 1 / 1, iteration: 1384 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1385 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1386 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1387 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1388 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1389 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1390 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1391 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1392 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1393 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1394 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1395 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1396 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 1397 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1398 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1399 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1400 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1401 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1402 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1403 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1404 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1405 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1406 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1407 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1408 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1409 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1410 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1411 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1412 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1413 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1414 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1415 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1416 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1417 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1418 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1419 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1420 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1421 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 1422 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1423 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1424 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1425 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1426 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1427 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1428 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1429 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1430 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1431 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1432 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1433 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1434 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1435 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1436 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1437 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1438 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1439 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1440 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1441 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1442 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1443 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1444 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1445 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1446 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1447 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1448 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1449 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1450 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1451 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1452 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1453 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1454 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1455 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1456 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1457 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1458 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1459 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1460 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1461 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1462 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1463 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1464 / 2400 | train loss : 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1465 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1466 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1467 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1468 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1469 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1470 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1471 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1472 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1473 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1474 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1475 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1476 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1477 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1478 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1479 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1480 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1481 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1482 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1483 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1484 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1485 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1486 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1487 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1488 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1489 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1490 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1491 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1492 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1493 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1494 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1495 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1496 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1497 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1498 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1499 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1500 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 1501 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1502 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1503 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1504 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1505 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1506 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1507 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1508 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1509 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1510 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1511 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1512 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1513 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1514 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1515 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 1516 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1517 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1518 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1519 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1520 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1521 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1522 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1523 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1524 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1525 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1526 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1527 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1528 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1529 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1530 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1531 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1532 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1533 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1534 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1535 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1536 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1537 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1538 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1539 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1540 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1541 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1542 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1543 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1544 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1545 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1546 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1547 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1548 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1549 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1550 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1551 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1552 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1553 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1554 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1555 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1556 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1557 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1558 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1559 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1560 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1561 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1562 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1563 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 1564 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1565 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1566 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1567 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1568 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1569 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1570 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1571 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1572 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1573 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1574 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1575 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1576 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1577 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1578 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1579 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1580 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1581 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1582 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1583 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1584 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1585 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1586 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1587 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1588 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1589 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1590 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1591 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1592 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1593 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1594 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1595 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1596 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1597 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1598 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1599 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1600 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1601 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1602 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1603 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1604 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1605 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1606 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1607 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1608 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1609 / 2400 | train loss : 2.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1610 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1611 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1612 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1613 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1614 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1615 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1616 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1617 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1618 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1619 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1620 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1621 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1622 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1623 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1624 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1625 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1626 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1627 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1628 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1629 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1630 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1631 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1632 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1633 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1634 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1635 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1636 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1637 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1638 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1639 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1640 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1641 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1642 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 1643 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1644 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1645 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1646 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1647 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1648 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1649 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1650 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1651 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1652 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1653 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1654 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1655 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1656 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1657 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1658 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1659 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1660 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1661 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1662 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1663 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1664 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1665 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1666 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1667 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1668 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1669 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1670 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1671 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1672 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1673 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1674 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1675 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1676 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1677 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1678 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1679 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1680 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1681 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1682 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1683 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1684 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1685 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1686 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1687 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1688 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1689 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1690 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1691 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1692 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1693 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1694 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1695 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1696 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1697 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1698 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1699 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1700 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1701 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1702 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1703 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1704 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1705 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 1706 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1707 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1708 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1709 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1710 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1711 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1712 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1713 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1714 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1715 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1716 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1717 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1718 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1719 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1720 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1721 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1722 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1723 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1724 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1725 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1726 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 1727 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1728 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1729 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1730 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1731 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1732 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1733 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1734 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1735 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1736 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1737 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 1738 / 2400 | train loss : 2.48\n",
      "epoch: 1 / 1, iteration: 1739 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1740 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1741 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1742 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1743 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1744 / 2400 | train loss : 2.47\n",
      "epoch: 1 / 1, iteration: 1745 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1746 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1747 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1748 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1749 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1750 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1751 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1752 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1753 / 2400 | train loss : 2.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1754 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1755 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1756 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1757 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1758 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1759 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1760 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1761 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1762 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1763 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1764 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1765 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1766 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1767 / 2400 | train loss : 2.44\n",
      "epoch: 1 / 1, iteration: 1768 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 1769 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1770 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1771 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1772 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1773 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1774 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1775 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1776 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1777 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1778 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1779 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1780 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1781 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1782 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1783 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1784 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1785 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1786 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1787 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1788 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1789 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1790 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1791 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1792 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1793 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1794 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1795 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1796 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1797 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1798 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1799 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1800 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1801 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1802 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1803 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1804 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1805 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1806 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1807 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1808 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1809 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1810 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1811 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1812 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1813 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1814 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1815 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1816 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1817 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1818 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1819 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1820 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1821 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1822 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 1823 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1824 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1825 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1826 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1827 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1828 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1829 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1830 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1831 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1832 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1833 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1834 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1835 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1836 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1837 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1838 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1839 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1840 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1841 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1842 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1843 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1844 / 2400 | train loss : 2.48\n",
      "epoch: 1 / 1, iteration: 1845 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1846 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1847 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1848 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1849 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1850 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1851 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1852 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1853 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1854 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1855 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1856 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1857 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1858 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 1859 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1860 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1861 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1862 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1863 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1864 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1865 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1866 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1867 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 1868 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 1869 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1870 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1871 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1872 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1873 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 1874 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1875 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 1876 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1877 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1878 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1879 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1880 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1881 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1882 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1883 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1884 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1885 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1886 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1887 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1888 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1889 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1890 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1891 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1892 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1893 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1894 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1895 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1896 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1897 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1898 / 2400 | train loss : 2.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 1899 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 1900 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1901 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1902 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1903 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1904 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1905 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1906 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1907 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1908 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1909 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1910 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1911 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1912 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1913 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1914 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1915 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1916 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1917 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1918 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1919 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1920 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1921 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1922 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1923 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1924 / 2400 | train loss : 2.44\n",
      "epoch: 1 / 1, iteration: 1925 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1926 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1927 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1928 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1929 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1930 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1931 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1932 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1933 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1934 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1935 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1936 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 1937 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 1938 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1939 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1940 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1941 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1942 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1943 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1944 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1945 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 1946 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1947 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1948 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1949 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1950 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1951 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1952 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1953 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1954 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 1955 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1956 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1957 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1958 / 2400 | train loss : 2.44\n",
      "epoch: 1 / 1, iteration: 1959 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1960 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1961 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1962 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1963 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1964 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1965 / 2400 | train loss : 2.47\n",
      "epoch: 1 / 1, iteration: 1966 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1967 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1968 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1969 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1970 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1971 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1972 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1973 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1974 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1975 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 1976 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 1977 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 1978 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1979 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1980 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1981 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1982 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1983 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1984 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 1985 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1986 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1987 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1988 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 1989 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 1990 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 1991 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 1992 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 1993 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 1994 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 1995 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1996 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 1997 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 1998 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 1999 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2000 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2001 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2002 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2003 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2004 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2005 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2006 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2007 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2008 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2009 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2010 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 2011 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2012 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2013 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2014 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2015 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2016 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2017 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2018 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2019 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2020 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2021 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2022 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2023 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2024 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2025 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2026 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2027 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2028 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2029 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2030 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2031 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2032 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2033 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2034 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2035 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2036 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2037 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2038 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 2039 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2040 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2041 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 2042 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2043 / 2400 | train loss : 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 2044 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2045 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2046 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2047 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2048 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2049 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2050 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2051 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2052 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2053 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2054 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2055 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2056 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2057 / 2400 | train loss : 2.46\n",
      "epoch: 1 / 1, iteration: 2058 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2059 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2060 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2061 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2062 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2063 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2064 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2065 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2066 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2067 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2068 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2069 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2070 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2071 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2072 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2073 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2074 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2075 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2076 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2077 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2078 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2079 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2080 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2081 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2082 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2083 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2084 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2085 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2086 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2087 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2088 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2089 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2090 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2091 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2092 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2093 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2094 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2095 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2096 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2097 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2098 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2099 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2100 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2101 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2102 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2103 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2104 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2105 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2106 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2107 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2108 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2109 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2110 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2111 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2112 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2113 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 2114 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2115 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2116 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 2117 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2118 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2119 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2120 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2121 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2122 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2123 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2124 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2125 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2126 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2127 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2128 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2129 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2130 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2131 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2132 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2133 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2134 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2135 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2136 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2137 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2138 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2139 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2140 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2141 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2142 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2143 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2144 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2145 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2146 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2147 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2148 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2149 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2150 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2151 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2152 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2153 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2154 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2155 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 2156 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2157 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2158 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2159 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2160 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2161 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2162 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2163 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2164 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2165 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2166 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 2167 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2168 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2169 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2170 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2171 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2172 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2173 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 2174 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2175 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2176 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2177 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2178 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 2179 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2180 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2181 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2182 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2183 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2184 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2185 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2186 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2187 / 2400 | train loss : 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 2188 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2189 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2190 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2191 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2192 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2193 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2194 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2195 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2196 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2197 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 2198 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2199 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2200 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2201 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2202 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2203 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2204 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2205 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2206 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2207 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2208 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2209 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2210 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2211 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2212 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2213 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2214 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2215 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2216 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2217 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2218 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 2219 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2220 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2221 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2222 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2223 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2224 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2225 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2226 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2227 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 2228 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2229 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2230 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2231 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2232 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2233 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2234 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2235 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2236 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2237 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2238 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 2239 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2240 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 2241 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2242 / 2400 | train loss : 2.43\n",
      "epoch: 1 / 1, iteration: 2243 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2244 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2245 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2246 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2247 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2248 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2249 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2250 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2251 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2252 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2253 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2254 / 2400 | train loss : 2.45\n",
      "epoch: 1 / 1, iteration: 2255 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2256 / 2400 | train loss : 2.22\n",
      "epoch: 1 / 1, iteration: 2257 / 2400 | train loss : 2.48\n",
      "epoch: 1 / 1, iteration: 2258 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2259 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2260 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2261 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2262 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2263 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2264 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2265 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2266 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2267 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2268 / 2400 | train loss : 2.19\n",
      "epoch: 1 / 1, iteration: 2269 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2270 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2271 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2272 / 2400 | train loss : 2.21\n",
      "epoch: 1 / 1, iteration: 2273 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2274 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2275 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2276 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2277 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2278 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2279 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2280 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2281 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2282 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2283 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2284 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2285 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2286 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2287 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2288 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2289 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2290 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2291 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2292 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2293 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2294 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2295 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2296 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2297 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 2298 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2299 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2300 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2301 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2302 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2303 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2304 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 2305 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2306 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2307 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2308 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2309 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2310 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2311 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2312 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2313 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2314 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2315 / 2400 | train loss : 2.23\n",
      "epoch: 1 / 1, iteration: 2316 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2317 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2318 / 2400 | train loss : 2.18\n",
      "epoch: 1 / 1, iteration: 2319 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2320 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2321 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2322 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2323 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 2324 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2325 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2326 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2327 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2328 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2329 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2330 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2331 / 2400 | train loss : 2.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 1, iteration: 2332 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2333 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2334 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2335 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2336 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2337 / 2400 | train loss : 2.44\n",
      "epoch: 1 / 1, iteration: 2338 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2339 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2340 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2341 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2342 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2343 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2344 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2345 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2346 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2347 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2348 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2349 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2350 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2351 / 2400 | train loss : 2.4\n",
      "epoch: 1 / 1, iteration: 2352 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2353 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2354 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2355 / 2400 | train loss : 2.26\n",
      "epoch: 1 / 1, iteration: 2356 / 2400 | train loss : 2.27\n",
      "epoch: 1 / 1, iteration: 2357 / 2400 | train loss : 2.25\n",
      "epoch: 1 / 1, iteration: 2358 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2359 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2360 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2361 / 2400 | train loss : 2.19\n",
      "epoch: 1 / 1, iteration: 2362 / 2400 | train loss : 2.47\n",
      "epoch: 1 / 1, iteration: 2363 / 2400 | train loss : 2.35\n",
      "epoch: 1 / 1, iteration: 2364 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2365 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2366 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2367 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2368 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2369 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2370 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2371 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2372 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2373 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2374 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2375 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2376 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2377 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2378 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2379 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2380 / 2400 | train loss : 2.36\n",
      "epoch: 1 / 1, iteration: 2381 / 2400 | train loss : 2.34\n",
      "epoch: 1 / 1, iteration: 2382 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2383 / 2400 | train loss : 2.3\n",
      "epoch: 1 / 1, iteration: 2384 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2385 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2386 / 2400 | train loss : 2.42\n",
      "epoch: 1 / 1, iteration: 2387 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2388 / 2400 | train loss : 2.28\n",
      "epoch: 1 / 1, iteration: 2389 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2390 / 2400 | train loss : 2.24\n",
      "epoch: 1 / 1, iteration: 2391 / 2400 | train loss : 2.31\n",
      "epoch: 1 / 1, iteration: 2392 / 2400 | train loss : 2.37\n",
      "epoch: 1 / 1, iteration: 2393 / 2400 | train loss : 2.41\n",
      "epoch: 1 / 1, iteration: 2394 / 2400 | train loss : 2.38\n",
      "epoch: 1 / 1, iteration: 2395 / 2400 | train loss : 2.33\n",
      "epoch: 1 / 1, iteration: 2396 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2397 / 2400 | train loss : 2.32\n",
      "epoch: 1 / 1, iteration: 2398 / 2400 | train loss : 2.29\n",
      "epoch: 1 / 1, iteration: 2399 / 2400 | train loss : 2.39\n",
      "epoch: 1 / 1, iteration: 2400 / 2400 | train loss : 2.21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3層, 初期値ガウス分布、tanh使用の結果\n",
    "network = ScratchDeepNeuralNetworkClassifier()\n",
    "network.fit(X_train, y_train, X_val, y_val, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果の正解率: 0.1028\n"
     ]
    }
   ],
   "source": [
    "pred = network.predict(X_test, y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(f'予測結果の正解率: {metrics.accuracy_score(y_test, pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習はできてはいるが、今回の実装だとエポックを増やしてもlossの改善が見られなかった。<br>\n",
    "また、１次元畳み込み層を1層目としたが、フィルタサイズの設定を力業で行っている。<br>\n",
    "その為、入力データサイズの変化に対応できず、不便な実装になってしまっている。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
