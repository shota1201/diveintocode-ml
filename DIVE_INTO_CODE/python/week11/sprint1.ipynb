{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 論文読解入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1506.01497.pdf<br>\n",
    "https://arxiv.org/pdf/1312.6229.pdf<br>\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。<br>\n",
    "物体提案分野：<br>\n",
    "include those based on grouping super-pixels (e.g., Selective Search, CPMC, MCG)<br>\n",
    "those based on sliding windows (e.g., objectness in windows, EdgeBoxes)<br>\n",
    "物体検出を行う深層学習：<br>\n",
    "R-CNN<br>\n",
    "The R-CNN method trains CNNs end-to-end to classify the proposal regions into object categories or background.<br>\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。<br>\n",
    "the MultiBox method [27] uses k-means to generate 800 anchors, which are not translation invariant. So MultiBox does not guarantee that the same proposal is generated if an object is translated.<br>\n",
    "→MultiBox has a (4 + 1) × 800-dimensional fully-connected output layer<br>\n",
    "しかし、提案手法では<br>\n",
    "our method has a (4 + 2) × 9-dimensional convolutional output layer in the case of k = 9 anchors.<br>\n",
    "物体を囲うbox候補の数を比較すると、1/100程度に削減できており、結果としてパラメータの数も以下の様に変わる<br>\n",
    "our output layer has 2.8 × 104 parameters (512 × (4 + 2) × 9 for VGG-16)<br>\n",
    "two orders of magnitude fewer than\n",
    "MultiBox’s output layer that has 6.1 × 106 parameters (1536 × (4 + 1) × 800 for GoogleNet [34] in MultiBox [27]).<br>\n",
    "おおよそ半分のパラメータ数になっている。<br>\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。<br>\n",
    "one-stage detection: The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps.<br>\n",
    "two-stage proposal + detection: 本論文の提案手法\n",
    "\n",
    "The OverFeatとは？：分類・検出タスクを行う特徴抽出器の事<br>\n",
    "引用論文：OverFeat:Integrated Recognition, Localization and Detection using Convolutional Networks<br>\n",
    "引用文章：we release a feature extractor from our best model called OverFeat.<br>\n",
    "\n",
    "(4) RPNとは何か。<br>\n",
    "RPN(Region Proposal Network)は、任意のサイズの画像を入力とし、３つの物体候補の長方形のボックスと、それが物体かどうかを示すスコアを出力する。\n",
    "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular\n",
    "3 object proposals, each with an objectness score.<br>\n",
    "\n",
    "(5) RoIプーリングとは何か。<br>\n",
    "RoIプーリングとは：<br>\n",
    "maxプーリングを用いて、有効な関心領域内の特徴を、H×W（例えば7×7）の固定された空間的広がりを持つ小さな特徴マップに変換する<br>\n",
    "引用論文：R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.<br>\n",
    "引用文章：The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small fea- ture map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are inde- pendent of any particular RoI.<br>\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。<br>\n",
    "アンカー数は注目点毎に、スケールとアスペクト比で決定されており、デフォルトの設定ではスケール3, アスペクト比3で1注目点につき9個のアンカーが設定される。<br>\n",
    "The k proposals are param- eterized relative to k reference boxes, which we call anchors. An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W H k anchors in total.<br>\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。<br>\n",
    "PASCAL VOC 2007 detection benchmarkというデータセットと、PASCAL VOC 2012 benchmarkというデータセットを用いている。<br>\n",
    "mAPという評価指標で、従来手法と比較すると約2〜3%の改善が見られている。<br>\n",
    "\n",
    "PASCAL VOC benchmarkとは？：<br>\n",
    "引用論文：M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” 2007.<br>\n",
    "引用文章：This dataset consists of about 5k trainval images and 5k test images over 20 object categories.<br>\n",
    "\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
